### 爬虫的小项目

##### 文件与图片的下载

```
import requests

urls = ['http://t2.hddhhn.com/uploads/tu/201707/50/135.jpg']

# 比较小的文件下载
for url in urls:
    r = requests.get(url)
    filename = url.split('/')[-1]
    with open(filename,'wb') as f:
        f.write(r.content)


# 大文件下载
url = 'https://www.python.org/ftp/python/3.7.0/Python-3.7.0.tgz'
filename = url.split('/')[-1]
r = requests.get(url,stream=True)  #stream=True设置断点续传，一段一段下载
with open(filename,'wb') as f:
    #迭代下载过来的内容，以每次1M的容量保存数据
    for chunk in r.iter_content(chunk_size=1024):
        f.write(chunk)
```

##### requests使用代理

```
"""
代理IP:  西刺代理等提供有免费代理IP
测试当前所使用的IP:
1)  http://ip.chinaz.com/getip.aspx/
2) https://www.baidu.com/s?wd=ip
"""
import requests

url = 'https://www.baidu.com/s?wd=ip'
proxies = {
    'https':'https://218.60.8.83:3129',
}
headers = {
'User-Agent':"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:61.0) Gecko/20100101 Firefox/61.0"
}

# 使用代理IP发送请求
r = requests.get(url,headers=headers,proxies=proxies)
with open('baidu.html','wb') as f:
    f.write(r.content)
```

##### requests超时设置

```
import requests
from requests import ConnectTimeout

url = 'https://www.baidu.com/favicon.ico'
try:
    r = requests.get(url,timeout=0.01)
except ConnectTimeout as e:
    print('连接超时.')
else:
    print(r.content)
```

##### requests发送cookies

```
import requests

headers = {

}
cookies = {

}

url = 'https://www.baidu.com/favicon.ico'
requests.get(url,headers=headers,cookies=cookies)
```

##### 爬虫需要下载的库

```
pip install aiohttp
pip install lxml
pip install seleium
```
删除为了满足其他软件包的依赖而安装的，但现在不再需要的软件包。

```
sudo apt autoremove
```

