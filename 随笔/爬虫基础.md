### 爬虫基础

    base_url = 'https://www.poxiao.com'
    def fetch(url):
        res = requests.get(url)
        if res.status_code is 200:
            return res
        return None
    
    res = fetch(base_url)
    print(type(res.text))   	#输出结果 <class 'str'>
    print(type(res.content))    #输出结果 <class 'bytes'>

### 多线程

```
import threading
import random
import time
class Work(threading.Thread):
    """docstring for Work"""
    # def __init__(self, arg):
    #     super(Work, self).__init__()
    #     self.arg = arg
    def run(self):
        print("开始模拟下载.")
        download_time = random.randint(3,5)
        print(download_time)
        time.sleep(download_time)
        print("下载完成.")  
tasks =[]
for i in range(30):
    tasks.append(Work())

for task in tasks:
    task.start()
task1 = Work()
task2 = Work()
task1.start()
task2.start()
```

### 多进程

```
from multiprocessing import Process
import time,os
t1 = time.time()
def process01():
    # while True:
        print("process01 的pid是:",os.getpid())
        print("process01 start")
        time.sleep(3)
def process02():
    print("process02 的pid是:",os.getpid())
    print("process02 start")
    time.sleep(3)
# 创建进程列表
process_pool =[]

for i in range(98):
    process_pool.append(Process(target=process01))
    
process_pool = [Process(target=process01) for i in range(98)]
#创建进程
p1 = Process(target=process01)
p2 = Process(target=process02)
begin_time = time.time()
for process in process_pool:
    process.start()
end_time = time.time()
print('end_time-begin_time:',end_time-begin_time)

# 启动进程
p1.start()
p2.start()
for process in process_pool:
    process.join()

# p1 p2 执行完后再执行某些代码  (即阻塞进程)
p1.join()
p2.join()

t2 = time.time()
print(t2-t1)

# while True:
#     time.sleep(2)
#     print("当前主进程的pid是:",os.getpid())
```

### 协程

```
import asyncio
import time

# 异步协程函数   定义一个执行函数
async def job(t):
    print('start job',t) # 开始运行的时间
    await asyncio.sleep(t)  # 等待t秒，利用等待的时间做别的任务
    print('任务{0}花费了{1}秒'.format(t,t))

#创建多个协程任务，放入一个列表中
async def main(loop):
    # 创建任务，放入列表中
    tasks = [
        loop.create_task(job(2)),# 这个任务需要２秒
        loop.create_task(job(3)),
        loop.create_task(job(4))
    ]
  # tasks = [loop.create_task(job(t))  for t in range(3)] 写法等同于上面的列表，生成一个3个任务列表

    # 设置多个任务以异步的方式执行
    await asyncio.wait(tasks)

#计算异步协程的运行时间
t1 = time.time()  # 获得时间
#　获得事件循环
loop = asyncio.get_event_loop()
#直到事件循环结束
loop.run_until_complete(main(loop))
#关闭事件循环
loop.close()
t2 = time.time()
print('总共用的时间是：',t2-t1)
```

### 线程锁

```
import threading

def f():
    print(threading.current_thread)
    # 加锁
    lock.acquire()
    print('hello')
    # 释放锁
    lock.release()
# 创建锁
lock = threading.Lock()

t1 = threading.Thread(target=f)
thread_pool = []
for i in range(10):
    thread_pool.append(threading.Thread(target=f,name=i))

for t in thread_pool:
    t.start()

t1.start()
```

### 守护线程

```
import time
import threading

def  fetch(url):
    time.sleep(3)
    print(url)

def f(url):
    time.sleep(2)
    print(url)

t1 = threading.Thread(target=fetch,args=('http://www.baidu.com',))
t2 = threading.Thread(target=f,args=('http://www.so.com',))

t1.daemon = True  # daemon的默认值是False,即非守护线程 
t2.daemon = False  #主线程会等待非守护线程

t1.start()
t2.start()

print('hello')
```

### 简单的爬个电影链接并插入数据库

    import pymysql
    import requests
    import re
    from bs4 import BeautifulSoup
    base_url = 'https://www.poxiao.com'
    
    #获取网页内容
    def fetch(url):
        res = requests.get(url)
        if res.status_code is 200:
            return res.content
        return None
    
    #利用正则解析网页内容中的链接
    def parse_url(html):
        #获取html文档中的所有链接
        #html：网页的源代码
        #返回值：返回url列表
        pattern = re.compile(r'href="(/movie/[0-9]+.html)"')
        urls = pattern.findall(html)
        return urls

**def main():**
    #调用获取网页的函数获取到网页内容
        html_doc = fetch('https://www.poxiao.com')
        if html_doc is not None:
            with open('dytt.html','w') as f:
                f.write(html_doc)
        解析页面中所有的url
            urls = parse_url(str(html_doc))
            for url in urls:
                print(url)
        return urls

**def save_to_db():         #保存到数据库**
    db = pymysql.Connect(host='localhost', port=3306, user='root', password='dy123321', database='day01')
    cursor = db.cursor()
    urls = main()
    try:
        for u in urls:
            # cursor.execute('insert into movies(url) values(%s)',[u])
            cursor.execute('insert into movies(url) values(%s)', (u,))
            db.commit()
        print('插入数据成功')
    except:
        print('添加数据失败')
    db.close()
    if name == 'main':
    	save_to_db()

#### bs4使用

```
#要爬取的内容 <a href="/movie/44015.html" target="_blank">2017美国动作科幻片《时间陷阱》BD中字</a>
#安装  pip install bs4
def parse_url_by_bs4(html):
    from bs4 import BeautifulSoup
    soup = BeautifulSoup(html)
    #soup.find_all()
    links = soup.find_all(name='a')    #找到所有的a标签
    links = [link.get('href')  for link in links]  #获取href属性的值
    return links
```

### urllib.request的使用

```
import urllib.request

header = {'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1'}
url = 'https://www.thepaper.cn/'

#data默认为None，不传参时表示为GET请求，传参的话表示为POST请求
class Request:   
    def __init__(self, url, data=None, headers={},
                 origin_req_host=None, unverifiable=False,
                 method=None): 
# 最基本的获取网页
with urllib.request.urlopen('http://www.lagou.com')  as response:
    html = response.read()

# 创建一个请求对象
request = urllib.request.Request(url=url,headers=header)

# 使用请求对象发送请求，返回一个类文件对象
with urllib.request.urlopen(request) as response:
    html = response.read().decode('utf-8')
    print(response.status_code)  #输出结果 200 表示请求成功
print(html)
```

### 模拟发送POST请求，需使用parse对data进行编码

```
import urllib.request
import urllib.parse

url = 'http://127.0.0.1:8000/admin/login/?next=/admin/'
data1 = {
    "csrfmiddlewaretoken":"tDZZ6KYOtSG7Jeu8QGQuuqLMC9yL5u9u8o7Z1qEs7idHdFlCcHHtFjYOBvnX3URq",
    "username":"admin",
    "password":"sunwenquan",
    'wq':'千峰'
}

# 对提交的数据进行编码
data = urllib.parse.urlencode(data1).encode('utf-8')
# 对提交的数据进行解码，此代码不执行
#num1 = urllib.parse.unquote(data)


#创建一个post请求对象，将编码后的data传入
req = urllib.request.Request(url=url,data=data)
#使用请求对象发送请求，返回一个类文件对象
with urllib.request.urlopen(req) as response:
     html = response.read()
#
# with open('demo.html','wb') as f:
#     f.write(html)
```

### urllib模拟登陆，保存服务器返回的cookie信息(难点)

```
"""
登录的步骤:
1. 获得登录页面(GET请求)  如何保存服务器返回的cookie信息?  使用 cookiejar
2. 提交登录信息(POST请求)
"""
import urllib.request
import urllib.parse
from http import cookiejar

url = 'http://192.168.141.130:8000/admin/login/?next=/admin/'
headers = {
    'username':'admin',
    'password':'sunwenquan'
}
# 创建一个cookie对象
cookie = cookiejar.CookieJar()
# 创建一个cookie处理器对象
processor = urllib.request.HTTPCookieProcessor(cookie)
# 创建一个发送请求的对象  
opener = urllib.request.build_opener(processor)

#使用请求对象发送请求，返回一个响应对象
resp = opener.open(url)
# print(type(resp))   <class 'http.client.HTTPResponse'>

#发送请求后，获取csrf值，并保存到请求头字典中   ------------------------------------------------------------
for item in cookie:
    if item.name == 'csrftoken':
        headers['csrfmiddlewaretoken'] = item.value
    print(item)  #打印结果 item相当于是一个对象
    <Cookie csrftoken=QbE7r6wjO9ZOGEzn5hvEGocjZieGvbNo23bR2TvmDbgFt27kaCbQIhaQeZtLvpXJ for 192.168.141.130/>
    print(item.value) #打印结果 80jEBmG3rBwjLi15zN3KuxXKzwnCZIcRJIquwYn1EhqIxFliXc12BKbI4JwIwtH8
    print(item.name)  #打印结果 csrftoken
#cookie的打印结果
<CookieJar[<Cookie csrftoken=1xLWQGCFdyCn1cAwhJ2Y0OfSyz0v7uENLfOgZYFl3yRpf1ciNn0W4MUG1Ooip96h for 192.168.141.130/>]>

# 对提交的数据进行编码
data = urllib.parse.urlencode(headers).encode('utf-8')
# print(data)

#创建一个post请求对象
req = urllib.request.Request(url=url,data=data)

resposne=opener.open(req)
with open('admin.html','wb')  as f:
    f.write(resposne.read())
```
### 爬虫获取网页的几种方法

##### urllib.request

```
import urllib.request

header = {'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1'}
url = 'https://www.thepaper.cn/'

#data默认为None，不传参时表示为GET请求，传参的话表示为POST请求
class Request:   
    def __init__(self, url, data=None, headers={},
                 origin_req_host=None, unverifiable=False,
                 method=None): 

#最基本的获取网页
url = 'http://www.baidu.com'
response = urllib.request.urlopen(url)  #返回一个类文件对象

#伪装请求头获取网页
header = {
'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1'
}
#创建请求对象
req = urllib.request.Request(url,headers=header)
#请求网页，获取响应对象
with urllib.request.urlopen(req) as response:
	http_doc = response.read()    #<class 'bytes'> 
	#解码成str
	http_doc = response.read().decode()  #<class 'str'>
```

##### requests(和如何发送post请求)

```
import requests
url = 'http://www.baidu.com'
#直接请求网页，获取响应对象
res = requests.get(url)
print(res)                 	#<Response [200]>
print(type(res))    		#<class 'requests.models.Response'> 响应对象
print(res.status_code)   	# 200

#伪装请求头获取网页
header = {
'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1'
}
res = requests.get(url=url,headers=header)

#获取网页内容的两种方式
print(res.text))     	  
print(res.content)  

print(type(res.text))      	#<class 'str'>  
print(type(res.content))   	#<class 'bytes'>

#requests发送post请求
import requests
url  = ''
data = {
    'username':'admin',
    'password':'123456'
}
headers = {
    'User-Agent':''
}
requests.post(url=url,data=data,headers=headers)
```

### 爬虫查找网页元素的几种方法

##### lxml.xpath

```
from lxml import etree
import requests
'''
#获取网页内容
html_etree = etree.parse('可以获取本地html文件')
#创建树级根节点对象
html_etree = etree.HTML('网上获取的html字符串(也可以是字节类型)')
#根据节点查找内容
html_etree.xpath('xpath路径')
'''
url = 'https://book.qidian.com/info/1012436593#Catalog'

req = requests.get(url)

tree = etree.HTML(req.content)  # <class 'lxml.etree._Element'>
#如果xpaht表达式有返回元素的话，总是返回一个数组，即使只有一个元素
catelogs = tree.xpath('//*[@id="j-catalogWrap"]/div[2]/div/ul/li')   #<class 'list'>
#遍历列表获取到的每一个对象都是Element类型，etree._Element 是一个设计很精妙的结构，可以把他当做一个对象访问当前节点自身的文本节点，可以把他当做一个数组，元素就是他的子节点，可以把它当做一个字典，从而遍历他的属性
for catelog in catelogs:   
    print(type(catelog))        # <class 'lxml.etree._Element'>
    
#查询
#包含查询
p_list = content_tree.xpath('//div[contains(@class, "read-content")]/p')
#属性值查询
link = 'https:'+catalog.xpath('.//a/@href')[0]
#文本查询
title = str(catalog.xpath('.//a/text()')[0])
```

##### lxml   应用：爬小说(难点)

```
#安装 pip install lxml
import requests
from lxml import etree

url = 'https://book.qidian.com/info/1012436593#Catalog'

r = requests.get(url)
# with open('qidian.html','wb') as f:
#     f.write(r.content)
#
# '//*[@id="j-catalogWrap"]/div[2]/div/ul/li'  所有章节对应的xpath

#创建对象
tree = etree.HTML(r.content)
print(type(tree))  #<class 'lxml.etree._Element'>

#etree._Element 是一个设计很精妙的结构，可以把他当做一个对象访问当前节点自身的文本节点，可以把他当做一个数组，元素就是他的子节点，可以把它当做一个字典，从而遍历他的属性

#处理数据，获取小说目录列表
catalogs = tree.xpath('//*[@id="j-catalogWrap"]/div[2]/div/ul/li')
print(type(catalogs))   <class 'list'>
f = open('qidian.txt','w')
'''< a
 href = "//read.qidian.com/chapter/XQn7qPeGb6gWXXlYBroA6g2/wChNtO8hjo1OBDFlr9quQA2"
 target = "_blank"
 data - eid = "qd_G55"
 data - cid = "//read.qidian.com/chapter/XQn7qPeGb6gWXXlYBroA6g2/wChNtO8hjo1OBDFlr9quQA2"
 title = "首发时间：2018-07-05 21:52:11 章节字数：2092" > 第5章
 我就打脸 < / a >'''
for catalog in catalogs[1:3]:
    try:
        print(type(catalog))   <class 'lxml.etree._Element'>
        title = str(catalog.xpath('.//a/text()')[0])   -------------------------------------------------
        print(title) #<class 'lxml.etree._Element'>
        link = 'https:'+catalog.xpath('.//a/@href')[0]
        print(link)
    except Exception as e:
        print('采集出错.')
        continue
    else:
        print('开始爬取章节:',title)

    # 获得content  某个章节的内容
    r = requests.get(link)
    content_tree = etree.HTML(r.content)
    # //div[@class="read-content"]/text()
    # //div[contains(@class, "read-content")]
    p_list = content_tree.xpath('//div[contains(@class, "read-content")]/p')
    content = ''
    for p in p_list:
        content +=str(p.xpath('.//text()')[0])+'\n'
    f.write(title+'\n')
    f.write(link+'\n')
    f.write(content)
f.close()
```

##### 正则解析

##### bs4解析

```
#要爬取的内容 <a href="/movie/44015.html" target="_blank">2017美国动作科幻片《时间陷阱》BD中字</a>
#安装  pip install bs4
def parse_url_by_bs4(html):
    from bs4 import BeautifulSoup
    soup = BeautifulSoup(html)
    #soup.find_all()
    links = soup.find_all(name='a')    #找到所有的a标签
    links = [link.get('href')  for link in links]  #获取href属性的值
    return links
```

### selenium的使用

npm 淘宝镜像

```
#安装
pip install selenium
from selenium import webdriver
url = 'https://search.jd.com/Search?keyword=%E6%89%8B%E6%9C%BA&enc=utf-8&wq=%E6%89%8B%E6%9C%BA&pvid=c022b77143a94dcda43044c16f624088'
#指定驱动创建对象 
driver = webdriver.Firefox() #使用火狐浏览器 需下载geckodriver并放置目录/usr/local/bin
driver = webdriver.Chrome()	 #使用谷歌浏览器 需下载chromedriver并放置目录/usr/local/bin
#请求url
driver.get(url)
#获取网页内容
response = driver.page_source
print(response)
```

##### selenium无界面运行

```
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

url = 'https://search.jd.com/Search?keyword=%E6%89%8B%E6%9C%BA&enc=utf-8&wq=%E6%89%8B%E6%9C%BA&pvid=c022b77143a94dcda43044c16f624088'
# 创建options对象并添加参数
options = Options()
options.add_argument('--headless')
#为chromeDriver的运行添加无界面运行的参数
driver = webdriver.Chrome(chrome_options=options)

#请求url
driver.get(url)

#把str以utf-8编码写入文件
with open('phone.html','wb') as f:
    f.write(driver.page_source.encode())

print(type(driver.page_source))  #<class 'str'>

# driver.close()


```